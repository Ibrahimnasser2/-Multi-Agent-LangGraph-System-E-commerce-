{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG system**"
      ],
      "metadata": {
        "id": "uZEbgD_u3oLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö install Required Libraries"
      ],
      "metadata": {
        "id": "Z0KNUppREqem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community pypdf faiss-cpu huggingface_hub transformers"
      ],
      "metadata": {
        "id": "rfFiE8ZVjqG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîê API Key Setup\n",
        "Set your API keys for OpenAI and Hugging Face. These keys allow access to large language models and embeddings."
      ],
      "metadata": {
        "id": "uVmzwewS36Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "iVLC8VXB3Zw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è RAGManagerFAISS Class Definition\n",
        "This class manages the entire Retrieval-Augmented Generation process. It loads, splits, indexes documents, and answers user queries."
      ],
      "metadata": {
        "id": "ZviV8POB4JIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# NEW: Imports for local LLM\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "class RAGManagerFAISS:\n",
        "    def __init__(self, embedding_model=None, faiss_index_path=\"faiss_index\", llm_model_name=\"google/flan-t5-large\"):\n",
        "        # Use HuggingFace sentence embeddings by default\n",
        "        self.embedding_model = embedding_model or HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "        # FAISS index path\n",
        "        self.faiss_index_path = faiss_index_path\n",
        "        self.faiss_store = None\n",
        "\n",
        "        # Load existing FAISS index if available\n",
        "        self._load_index()\n",
        "\n",
        "        # Load local LLM model for QA generation (FLAN-T5)\n",
        "        print(f\"üîß Loading local LLM model: {llm_model_name} ...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
        "\n",
        "        # Create pipeline for text2text generation (QA)\n",
        "        self.qa_pipeline = pipeline(\"text2text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
        "\n",
        "        print(\"‚úÖ LLM model loaded successfully!\")\n",
        "\n",
        "    def load_documents(self, file_paths=[], urls=[]):\n",
        "        documents = []\n",
        "\n",
        "        # Load PDFs\n",
        "        for path in file_paths:\n",
        "            loader = PyPDFLoader(path)\n",
        "            docs = loader.load()\n",
        "            documents.extend(docs)\n",
        "\n",
        "        # Load URLs\n",
        "        for url in urls:\n",
        "            loader = WebBaseLoader(url)\n",
        "            docs = loader.load()\n",
        "            documents.extend(docs)\n",
        "\n",
        "        print(f\"üìÑ Loaded {len(documents)} documents.\")\n",
        "        return documents\n",
        "\n",
        "    def split_documents(self, documents, chunk_size=1000, chunk_overlap=200):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        chunks = splitter.split_documents(documents)\n",
        "        print(f\"‚úÇÔ∏è Split documents into {len(chunks)} chunks.\")\n",
        "        return chunks\n",
        "\n",
        "    def index_documents(self, documents):\n",
        "        # Split into chunks first\n",
        "        chunks = self.split_documents(documents)\n",
        "\n",
        "        if self.faiss_store is None:\n",
        "            # Create a new FAISS index\n",
        "            self.faiss_store = FAISS.from_documents(chunks, self.embedding_model)\n",
        "            print(f\"‚úÖ Created new FAISS index with {len(chunks)} chunks.\")\n",
        "        else:\n",
        "            # Add new documents to existing index\n",
        "            self.faiss_store.add_documents(chunks)\n",
        "            print(f\"‚ûï Added {len(chunks)} chunks to existing FAISS index.\")\n",
        "\n",
        "        # Save the updated index\n",
        "        self._save_index()\n",
        "\n",
        "    def query(self, query, top_k=1):\n",
        "        if self.faiss_store is None:\n",
        "            raise ValueError(\"No FAISS index found. Please index documents first!\")\n",
        "\n",
        "        results = self.faiss_store.similarity_search(query, k=top_k)\n",
        "\n",
        "        # Return the content of matched docs\n",
        "        return [result.page_content for result in results]\n",
        "\n",
        "    def generate_answer(self, question, top_k=3):\n",
        "        if self.faiss_store is None:\n",
        "            raise ValueError(\"No indexed documents to generate answers from.\")\n",
        "\n",
        "        # Retrieve top_k most similar documents\n",
        "        retrieved_docs = self.faiss_store.similarity_search(question, k=top_k)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            print(\"‚ö†Ô∏è No documents retrieved for the given question.\")\n",
        "            return \"No relevant information found.\"\n",
        "\n",
        "        # Combine retrieved documents into context\n",
        "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "        # Format the input prompt for the LLM\n",
        "        prompt = f\"Question: {question}\\nContext: {context}\"\n",
        "\n",
        "        # Generate answer using the local pipeline\n",
        "        output = self.qa_pipeline(prompt, max_length=256, temperature=0.2)\n",
        "\n",
        "        answer = output[0]['generated_text']\n",
        "        print(f\"ü§ñ Generated Answer: {answer}\")\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def _save_index(self):\n",
        "        \"\"\"Save FAISS index to local directory.\"\"\"\n",
        "        if self.faiss_store is None:\n",
        "            print(\"‚ö†Ô∏è No FAISS index to save.\")\n",
        "            return\n",
        "\n",
        "        os.makedirs(self.faiss_index_path, exist_ok=True)\n",
        "\n",
        "        # Save FAISS index to disk\n",
        "        self.faiss_store.save_local(self.faiss_index_path)\n",
        "        print(f\"üíæ FAISS index saved at: {self.faiss_index_path}\")\n",
        "\n",
        "    def _load_index(self):\n",
        "        \"\"\"Load FAISS index from local directory if exists.\"\"\"\n",
        "        if not os.path.exists(self.faiss_index_path):\n",
        "            print(\"‚ÑπÔ∏è No existing FAISS index found. Starting fresh.\")\n",
        "            self.faiss_store = None\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.faiss_store = FAISS.load_local(\n",
        "                self.faiss_index_path,\n",
        "                self.embedding_model,\n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "            print(f\"‚úÖ Loaded FAISS index from: {self.faiss_index_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load FAISS index: {e}\")\n",
        "            self.faiss_store = None\n",
        "\n",
        "# ===========================\n",
        "# ‚úÖ Example usage\n",
        "# ===========================\n",
        "if __name__ == \"__main__\":\n",
        "    # Instantiate the RAG Manager (no need for HF token!)\n",
        "    rag_manager = RAGManagerFAISS()\n",
        "\n",
        "    # 1. Load documents (PDF or URL)\n",
        "    docs = rag_manager.load_documents(file_paths=[\"/content/Ibrahim_Nasser_Darwish_Mostafa_CV.pdf\"])\n",
        "\n",
        "    # 2. Index those documents (or skip if already indexed)\n",
        "    rag_manager.index_documents(docs)\n",
        "\n",
        "    # 3. Retrieve docs with a query\n",
        "    query = \"What is the skills of the person ?\"\n",
        "    results = rag_manager.query(query, top_k=2)\n",
        "\n",
        "    print(\"\\nüîé Retrieved Docs:\")\n",
        "    for res in results:\n",
        "        print(res)\n",
        "\n",
        "    # 4. Generate an answer from the top documents\n",
        "    answer = rag_manager.generate_answer(query, top_k=2)\n",
        "\n",
        "    print(\"\\nüí° Final Answer:\")\n",
        "    print(answer)\n"
      ],
      "metadata": {
        "id": "2eJDp8bf9ie0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1944e6e5-8485-408e-9edc-a8e4959451b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded FAISS index from: faiss_index\n",
            "üîß Loading local LLM model: google/flan-t5-large ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LLM model loaded successfully!\n",
            "üìÑ Loaded 2 documents.\n",
            "‚úÇÔ∏è Split documents into 6 chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ûï Added 6 chunks to existing FAISS index.\n",
            "üíæ FAISS index saved at: faiss_index\n",
            "\n",
            "üîé Retrieved Docs:\n",
            "2  \n",
            "SKILLS  \n",
            "‚Ä¢ ‚Ä¢  Problem Solving Skills  \n",
            "‚Ä¢ ‚Ä¢  C, C++, Java, java script, Dart, Python.  \n",
            "‚Ä¢ ‚Ä¢  Frameworks:  \n",
            "‚Ä¢      Scikit-Learn, Matplotlib, TensorFlow, Keras, \n",
            "‚Ä¢      OpenCV, PyTorch, LangChain, Angular, Streamlit, \n",
            "‚Ä¢      Flutter. \n",
            "‚Ä¢ ‚Ä¢  LLMs and Generative AI.  \n",
            "‚Ä¢ ‚Ä¢  Power BI.  \n",
            "‚Ä¢ ‚Ä¢  Database (My SQL, SQLite, MongoDB).  \n",
            "‚Ä¢ ‚Ä¢  Contributor in Kaggle with many notebooks. \n",
            "‚Ä¢ API with Postman. \n",
            "‚Ä¢ Dio & Http. \n",
            "‚Ä¢ Firebase. \n",
            "‚Ä¢ Microsoft offices & MATLAB. \n",
            "‚Ä¢ Teamwork. \n",
            "‚Ä¢ Organization Skills. \n",
            "‚Ä¢ Attention to Detail. \n",
            "‚Ä¢ Critical Thinking Skills. \n",
            "‚Ä¢ Docker Deployment. \n",
            " \n",
            "LANGUAGE \n",
            "‚Ä¢ English (very good). \n",
            "‚Ä¢ Italian (fair knowledge). \n",
            "‚Ä¢ Arabic (native). \n",
            " \n",
            " \n",
            "‚ùñ \n",
            "‚Ä¢ Developed a task management (To-Do) application using a local database for offline \n",
            "functionality. \n",
            "‚Ä¢ Built a BIM calculator application to assist in weight and fat computations. \n",
            "‚Ä¢ Designed a public news aggregation application for real-time site updates.\n",
            "2  \n",
            "SKILLS  \n",
            "‚Ä¢ ‚Ä¢  Problem Solving Skills  \n",
            "‚Ä¢ ‚Ä¢  C, C++, Java, java script, Dart, Python.  \n",
            "‚Ä¢ ‚Ä¢  Frameworks:  \n",
            "‚Ä¢      Scikit-Learn, Matplotlib, TensorFlow, Keras, \n",
            "‚Ä¢      OpenCV, PyTorch, LangChain, Angular, Streamlit, \n",
            "‚Ä¢      Flutter. \n",
            "‚Ä¢ ‚Ä¢  LLMs and Generative AI.  \n",
            "‚Ä¢ ‚Ä¢  Power BI.  \n",
            "‚Ä¢ ‚Ä¢  Database (My SQL, SQLite, MongoDB).  \n",
            "‚Ä¢ ‚Ä¢  Contributor in Kaggle with many notebooks. \n",
            "‚Ä¢ API with Postman. \n",
            "‚Ä¢ Dio & Http. \n",
            "‚Ä¢ Firebase. \n",
            "‚Ä¢ Microsoft offices & MATLAB. \n",
            "‚Ä¢ Teamwork. \n",
            "‚Ä¢ Organization Skills. \n",
            "‚Ä¢ Attention to Detail. \n",
            "‚Ä¢ Critical Thinking Skills. \n",
            "‚Ä¢ Docker Deployment. \n",
            " \n",
            "LANGUAGE \n",
            "‚Ä¢ English (very good). \n",
            "‚Ä¢ Italian (fair knowledge). \n",
            "‚Ä¢ Arabic (native). \n",
            " \n",
            " \n",
            "‚ùñ \n",
            "‚Ä¢ Developed a task management (To-Do) application using a local database for offline \n",
            "functionality. \n",
            "‚Ä¢ Built a BIM calculator application to assist in weight and fat computations. \n",
            "‚Ä¢ Designed a public news aggregation application for real-time site updates.\n",
            "ü§ñ Generated Answer: 2 SKILLS ‚Ä¢ ‚Ä¢ Problem Solving Skills ‚Ä¢ ‚Ä¢ C, C++, Java, java script, Dart, Python. ‚Ä¢ ‚Ä¢ Frameworks: ‚Ä¢ Scikit-Learn, Matplotlib, TensorFlow, Keras, ‚Ä¢ OpenCV, PyTorch, LangChain, Angular, Streamlit, ‚Ä¢ Flutter. ‚Ä¢ ‚Ä¢ LLMs and Generative AI. ‚Ä¢ ‚Ä¢ Power BI. ‚Ä¢ ‚Ä¢ Database (My SQL, SQLite, MongoDB). ‚Ä¢ ‚Ä¢ Contributor in Kaggle with many notebooks. ‚Ä¢ API with Postman. ‚Ä¢ Dio & Http. ‚Ä¢ Firebase. ‚Ä¢ Microsoft offices & MATLAB. ‚Ä¢ Teamwork. ‚Ä¢ Organization Skills. ‚Ä¢ Attention to Detail. ‚Ä¢ Critical Thinking Skills. ‚Ä¢ Docker Deployment. LANGUAGE ‚Ä¢ English (very good). ‚Ä¢ Italian (fair knowledge). ‚Ä¢ Arabic (native).  ‚Ä¢ Developed a task management (To-Do) application using a local database for offline functionality. ‚Ä¢ Built a BIM calculator application to assist in weight and fat computations. ‚Ä¢ Designed a public news aggregat\n",
            "\n",
            "üí° Final Answer:\n",
            "2 SKILLS ‚Ä¢ ‚Ä¢ Problem Solving Skills ‚Ä¢ ‚Ä¢ C, C++, Java, java script, Dart, Python. ‚Ä¢ ‚Ä¢ Frameworks: ‚Ä¢ Scikit-Learn, Matplotlib, TensorFlow, Keras, ‚Ä¢ OpenCV, PyTorch, LangChain, Angular, Streamlit, ‚Ä¢ Flutter. ‚Ä¢ ‚Ä¢ LLMs and Generative AI. ‚Ä¢ ‚Ä¢ Power BI. ‚Ä¢ ‚Ä¢ Database (My SQL, SQLite, MongoDB). ‚Ä¢ ‚Ä¢ Contributor in Kaggle with many notebooks. ‚Ä¢ API with Postman. ‚Ä¢ Dio & Http. ‚Ä¢ Firebase. ‚Ä¢ Microsoft offices & MATLAB. ‚Ä¢ Teamwork. ‚Ä¢ Organization Skills. ‚Ä¢ Attention to Detail. ‚Ä¢ Critical Thinking Skills. ‚Ä¢ Docker Deployment. LANGUAGE ‚Ä¢ English (very good). ‚Ä¢ Italian (fair knowledge). ‚Ä¢ Arabic (native).  ‚Ä¢ Developed a task management (To-Do) application using a local database for offline functionality. ‚Ä¢ Built a BIM calculator application to assist in weight and fat computations. ‚Ä¢ Designed a public news aggregat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  ‚úÖchecking for  getting from web pages"
      ],
      "metadata": {
        "id": "4DIPiWb65CnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    docs = rag_manager.load_documents(urls=[\"https://github.com/Ibrahimnasser2\"])\n",
        "\n",
        "    # 2. Index those documents (or skip if already indexed)\n",
        "    rag_manager.index_documents(docs)\n",
        "\n",
        "    # 3. Retrieve docs with a query\n",
        "    query = \"What is the name of the person ?\"\n",
        "    # 4. Generate an answer from the top documents\n",
        "    answer = rag_manager.generate_answer(query, top_k=2)\n",
        "\n",
        "    print(\"\\nüí° Final Answer:\")\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dytdBFJApI3W",
        "outputId": "19f0d847-4c21-4dad-bb40-afeb1bcf88a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Loaded 1 documents.\n",
            "‚úÇÔ∏è Split documents into 8 chunks.\n",
            "‚ûï Added 8 chunks to existing FAISS index.\n",
            "üíæ FAISS index saved at: faiss_index\n",
            "ü§ñ Generated Answer: Ibrahimnasser2\n",
            "\n",
            "üí° Final Answer:\n",
            "Ibrahimnasser2\n"
          ]
        }
      ]
    }
  ]
}