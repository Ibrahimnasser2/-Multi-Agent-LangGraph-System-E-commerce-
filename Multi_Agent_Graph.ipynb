{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Multi-Agent Graph with RAG & LangSmith Evaluation\n",
        "This notebook demonstrates a LangGraph multi-agent system with memory extraction, RAG QA, and summary nodes. It includes LangSmith evaluations for assessing accuracy and relevance\n"
      ],
      "metadata": {
        "id": "uZEbgD_u3oLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 install Required Libraries"
      ],
      "metadata": {
        "id": "Z0KNUppREqem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community pypdf faiss-cpu  langgraph langchain transformers"
      ],
      "metadata": {
        "id": "OFiV9-lYZe-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔐 API Key Setup\n",
        "Set your API keys for OpenAI and Hugging Face. These keys allow access to large language models and embeddings."
      ],
      "metadata": {
        "id": "uVmzwewS36Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "iVLC8VXB3Zw9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "LrERHaNGZmhn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##📦 3. Imports\n",
        "\n",
        "We import only the necessary modules."
      ],
      "metadata": {
        "id": "45DS4zY5kSnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import Graph\n",
        "from langsmith import Client\n",
        "from langsmith.evaluation import EvaluationResult\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from typing import Dict, Any, List\n",
        "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.callbacks import tracing_v2_enabled\n",
        "from langsmith.evaluation import EvaluationResult\n"
      ],
      "metadata": {
        "id": "6J3C28bnkRfc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏗️ RAGManagerFAISS Class Definition\n",
        "This class manages the entire Retrieval-Augmented Generation process. It loads, splits, indexes documents, and answers user queries."
      ],
      "metadata": {
        "id": "ZviV8POB4JIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGManagerFAISS:\n",
        "    def __init__(self, embedding_model=None, faiss_index_path=\"faiss_index\", llm_model_name=\"google/flan-t5-large\"):\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = embedding_model or HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "        # FAISS vector store setup\n",
        "        self.faiss_index_path = faiss_index_path\n",
        "        self.faiss_store = None\n",
        "        self._load_index()\n",
        "\n",
        "        # Initialize LLM (FLAN-T5 by default)\n",
        "        print(f\"🔧 Loading LLM model: {llm_model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
        "        self.qa_pipeline = pipeline(\"text2text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
        "        print(\"✅ LLM ready!\")\n",
        "\n",
        "    # ===========================\n",
        "    # Document Processing\n",
        "    # ===========================\n",
        "    def load_documents(self, file_paths=[], urls=[]):\n",
        "        documents = []\n",
        "        for path in file_paths:\n",
        "            loader = PyPDFLoader(path)\n",
        "            docs = loader.load()\n",
        "            documents.extend(docs)\n",
        "\n",
        "        for url in urls:\n",
        "            loader = WebBaseLoader(url)\n",
        "            docs = loader.load()\n",
        "            documents.extend(docs)\n",
        "\n",
        "        print(f\"📄 Loaded {len(documents)} documents.\")\n",
        "        return documents\n",
        "\n",
        "    def split_documents(self, documents, chunk_size=1000, chunk_overlap=200):\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        chunks = splitter.split_documents(documents)\n",
        "        print(f\"✂️ Split into {len(chunks)} chunks.\")\n",
        "        return chunks\n",
        "\n",
        "    def index_documents(self, documents):\n",
        "        chunks = self.split_documents(documents)\n",
        "        if self.faiss_store is None:\n",
        "            self.faiss_store = FAISS.from_documents(chunks, self.embedding_model)\n",
        "            print(f\"✅ Created new FAISS index with {len(chunks)} chunks.\")\n",
        "        else:\n",
        "            self.faiss_store.add_documents(chunks)\n",
        "            print(f\"➕ Added {len(chunks)} chunks to existing index.\")\n",
        "        self._save_index()\n",
        "\n",
        "    # ===========================\n",
        "    # Query & QA\n",
        "    # ===========================\n",
        "    def query(self, query_text, top_k=1):\n",
        "        if not self.faiss_store:\n",
        "            raise ValueError(\"No FAISS index found. Please index documents first!\")\n",
        "\n",
        "        results = self.faiss_store.similarity_search(query_text, k=top_k)\n",
        "        return [result.page_content for result in results]\n",
        "\n",
        "    def generate_answer(self, question, top_k=3):\n",
        "        if not self.faiss_store:\n",
        "            raise ValueError(\"No indexed documents to generate answers from.\")\n",
        "\n",
        "        retrieved_docs = self.faiss_store.similarity_search(question, k=top_k)\n",
        "        if not retrieved_docs:\n",
        "            return \"No relevant information found.\"\n",
        "\n",
        "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "        prompt = f\"Question: {question}\\nContext: {context}\"\n",
        "\n",
        "        output = self.qa_pipeline(prompt, max_length=256, temperature=0.2)\n",
        "        answer = output[0]['generated_text']\n",
        "        return answer\n",
        "\n",
        "    # ===========================\n",
        "    # Index Management\n",
        "    # ===========================\n",
        "    def _save_index(self):\n",
        "        if not self.faiss_store:\n",
        "            print(\"⚠️ No FAISS index to save.\")\n",
        "            return\n",
        "        os.makedirs(self.faiss_index_path, exist_ok=True)\n",
        "        self.faiss_store.save_local(self.faiss_index_path)\n",
        "        print(f\"💾 FAISS index saved at: {self.faiss_index_path}\")\n",
        "\n",
        "    def _load_index(self):\n",
        "        if not os.path.exists(self.faiss_index_path):\n",
        "            print(\"ℹ️ No existing FAISS index found.\")\n",
        "            return\n",
        "        try:\n",
        "            self.faiss_store = FAISS.load_local(self.faiss_index_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
        "            print(f\"✅ FAISS index loaded from: {self.faiss_index_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load FAISS index: {e}\")\n",
        "            self.faiss_store = None\n",
        "\n",
        "    # ===========================\n",
        "    # Node-Friendly Callables for LangGraph\n",
        "    # ===========================\n",
        "\n",
        "    # This is your LangGraph RAG node\n",
        "    def rag_query_node(self, state: dict) -> dict:\n",
        "        \"\"\"\n",
        "        LangGraph Node: Accepts state dict and returns updated state dict with RAG answer.\n",
        "        \"\"\"\n",
        "        # Extract the latest user question/message\n",
        "        question = state.get(\"messages\", [])[-1]\n",
        "        print(f\"🔎 RAG Node processing question: {question}\")\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.generate_answer(question, top_k=3)\n",
        "\n",
        "        # Append the answer back into the conversation/messages state\n",
        "        state[\"messages\"].append(answer)\n",
        "\n",
        "        # Return updated state for the next node\n",
        "        return state\n",
        "\n",
        "    def summarize_node(self, state: dict) -> dict:\n",
        "        \"\"\"\n",
        "        LangGraph Node: Summarizes conversation if messages exceed 25.\n",
        "        \"\"\"\n",
        "        messages = state.get(\"messages\", [])\n",
        "        print(f\"📋 Summary Node checking message length: {len(messages)}\")\n",
        "\n",
        "        if len(messages) > 25:\n",
        "            context = \"\\n\".join(messages)\n",
        "            prompt = f\"Summarize the following conversation:\\n{context}\"\n",
        "\n",
        "            summary_output = self.qa_pipeline(prompt, max_length=200)[0]['generated_text']\n",
        "            print(f\"📜 Summary: {summary_output}\")\n",
        "\n",
        "            # Keep only the summary\n",
        "            state[\"messages\"] = [summary_output]\n",
        "\n",
        "        return state\n"
      ],
      "metadata": {
        "id": "5TUv7YX8O7Pj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏗️ Multi-Agent Graph system"
      ],
      "metadata": {
        "id": "8V44HsoQbgvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import Graph\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# 1. Instantiate RAG Manager\n",
        "rag_manager = RAGManagerFAISS()\n",
        "docs = rag_manager.load_documents(file_paths=[\"/content/smartwatch.pdf\"])\n",
        "\n",
        "# 2. Index those documents (or skip if already indexed)\n",
        "rag_manager.index_documents(docs)\n",
        "\n",
        "# 2. Memory to store extracted user data\n",
        "user_memory = {}\n",
        "\n",
        "# --------------------\n",
        "# Memory Extraction Node\n",
        "# --------------------\n",
        "def memory_extraction_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        print(\"⚠️ No messages found in state.\")\n",
        "        return state\n",
        "\n",
        "    message = messages[-1]\n",
        "    print(f\"📝 Memory Extraction Node: Processing message -> {message}\")\n",
        "\n",
        "    user_message = message.get(\"content\", \"\") if isinstance(message, dict) else str(message)\n",
        "\n",
        "    if \"my name is\" in user_message.lower():\n",
        "        user_name = user_message.split(\"my name is\")[-1].strip().split()[0]\n",
        "        user_memory['name'] = user_name\n",
        "        print(f\"✅ Extracted name: {user_name}\")\n",
        "\n",
        "    state[\"memory\"] = user_memory\n",
        "    return state\n",
        "\n",
        "# --------------------\n",
        "# RAG Node (QA Agent)\n",
        "# --------------------\n",
        "def rag_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        print(\"⚠️ No messages found in state.\")\n",
        "        return state\n",
        "\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    # Extract user question as a string\n",
        "    user_question = last_message.get(\"content\", \"\") if isinstance(last_message, dict) else str(last_message)\n",
        "\n",
        "    print(f\"🔎 RAG Node: Received question -> {user_question}\")\n",
        "\n",
        "    # Pass the string to the RAG Manager\n",
        "    answer = rag_manager.generate_answer(user_question, top_k=3)\n",
        "\n",
        "    # Append the answer as a new message\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": answer\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "# --------------------\n",
        "# Summary Node\n",
        "# --------------------\n",
        "def summary_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    messages = state.get(\"messages\", [])\n",
        "    print(f\"📋 Summary Node triggered. Messages count: {len(messages)}\")\n",
        "\n",
        "    if len(messages) > 25:\n",
        "        # Extract content from message dicts\n",
        "        context = \"\\n\".join(\n",
        "            msg.get(\"content\", \"\") if isinstance(msg, dict) else str(msg)\n",
        "            for msg in messages\n",
        "        )\n",
        "\n",
        "        prompt = f\"Summarize the following conversation:\\n{context}\"\n",
        "\n",
        "        summary = rag_manager.qa_pipeline(prompt, max_length=200)[0]['generated_text']\n",
        "        print(f\"📜 Summary: {summary}\")\n",
        "\n",
        "        state[\"messages\"] = [{\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": summary\n",
        "        }]\n",
        "\n",
        "    return state\n",
        "\n",
        "# --------------------\n",
        "# Graph Setup\n",
        "# --------------------\n",
        "graph = Graph()\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"MemoryExtraction\", memory_extraction_node)\n",
        "graph.add_node(\"RAGQA\", rag_node)\n",
        "graph.add_node(\"Summary\", summary_node)\n",
        "\n",
        "# Define edges\n",
        "graph.add_edge(\"MemoryExtraction\", \"RAGQA\")\n",
        "graph.add_edge(\"RAGQA\", \"Summary\")\n",
        "\n",
        "graph.set_entry_point(\"MemoryExtraction\")\n",
        "graph.set_finish_point(\"Summary\")\n",
        "\n",
        "# --------------------\n",
        "# Graph Example Execution\n",
        "# --------------------\n",
        "# Mock messages (example state)\n",
        "state = {\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Hi, what is smartwatch?\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "runnable_graph = graph.compile()\n",
        "\n",
        "# Run the graph\n",
        "final_state = runnable_graph.invoke(state)\n",
        "\n",
        "# Final output\n",
        "print(\"\\n🛠️ Final State:\")\n",
        "print(final_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0q3gsvFO7Sc",
        "outputId": "1d374520-867c-4072-e5f5-8f5428dd1005"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8d9d854b2824>:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  self.embedding_model = embedding_model or HuggingFaceEmbeddings(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index loaded from: faiss_index\n",
            "🔧 Loading LLM model: google/flan-t5-large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LLM ready!\n",
            "📄 Loaded 20 documents.\n",
            "✂️ Split into 64 chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➕ Added 64 chunks to existing index.\n",
            "💾 FAISS index saved at: faiss_index\n",
            "📝 Memory Extraction Node: Processing message -> {'role': 'user', 'content': 'Hi, what is smartwatch?'}\n",
            "🔎 RAG Node: Received question -> Hi, what is smartwatch?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Summary Node triggered. Messages count: 2\n",
            "\n",
            "🛠️ Final State:\n",
            "{'messages': [{'role': 'user', 'content': 'Hi, what is smartwatch?'}, {'role': 'assistant', 'content': 'A smartwatch is a portable wearable computer that resembles a wristwatch'}], 'memory': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🏗️ Graph Architecture"
      ],
      "metadata": {
        "id": "FtWiFSvNrXAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runnable_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Jou-0rDvbr_A",
        "outputId": "eeeabc23-dce9-4d75-d177-b366578577e7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.graph.CompiledGraph object at 0x7b926911ce50>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##✅Checking for Summary Node work"
      ],
      "metadata": {
        "id": "m-tAXi69vKjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    \"What is a smartwatch?\",\n",
        "    \"How do modern smartwatches connect to other devices?\",\n",
        "    \"Tell me about the first smartwatches ever made.\",\n",
        "    \"Which smartwatch was first capable of storing data?\",\n",
        "    \"What features does a modern smartwatch offer?\",\n",
        "    \"Can smartwatches make phone calls?\",\n",
        "    \"What type of screens do smartwatches use?\",\n",
        "    \"Do smartwatches have GPS and sensors?\",\n",
        "    \"Are there fitness tracking features in smartwatches?\",\n",
        "    \"What was the Pulsar watch?\",\n",
        "    \"How did Seiko improve smartwatches in the 1980s?\",\n",
        "    \"What was the Seiko Data 2000?\",\n",
        "    \"Tell me about Seiko’s RC series.\",\n",
        "    \"What was special about the RC-1000 Wrist Terminal?\",\n",
        "    \"Which computers did the RC-1000 connect with?\",\n",
        "    \"What were Casio game watches?\",\n",
        "    \"What was the Timex Datalink and what did it do?\",\n",
        "    \"Who created the first Linux wristwatch?\",\n",
        "    \"What was the Ruputer and why did it fail?\",\n",
        "    \"What was Samsung's SPH-WP10 watch phone?\",\n",
        "    \"When did IBM show its WatchPad prototype?\",\n",
        "    \"What were the features of IBM WatchPad 1.5?\",\n",
        "    \"How did Fossil Wrist PDA work?\",\n",
        "    \"What was Microsoft’s SPOT smartwatch?\",\n",
        "    \"What was Sony Ericsson’s MBW-100 watch?\",\n",
        "    \"Who is Burg and what did they contribute to smartwatch history?\",\n",
        "    \"What is special about Samsung’s S9110 Watch Phone?\",\n",
        "    \"Describe the Sony Ericsson LiveView.\",\n",
        "    \"What features did Amazfit Bip have?\",\n",
        "    \"Describe the Gear 2 smartwatch by Samsung.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "5E1R_IwNvBqv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\n",
        "    \"messages\": messages\n",
        "}\n",
        "runnable_graph = graph.compile()\n",
        "# Run the graph with the state (simulate a conversation flow)\n",
        "final_state = runnable_graph.invoke(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMBHgXWSvByU",
        "outputId": "9d6ff7e7-adc4-4251-ef3b-574244112ad9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Memory Extraction Node: Processing message -> Describe the Gear 2 smartwatch by Samsung.\n",
            "🔎 RAG Node: Received question -> Describe the Gear 2 smartwatch by Samsung.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Summary Node triggered. Messages count: 31\n",
            "📜 Summary: Describe the history of smartwatches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#📝Evaluation Part"
      ],
      "metadata": {
        "id": "m4RXI_aebylj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔐 API Key Setup"
      ],
      "metadata": {
        "id": "jItvxDd5cPm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n"
      ],
      "metadata": {
        "id": "T8cKO5ebPeuj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##📏 10. Evaluation Functions"
      ],
      "metadata": {
        "id": "xeS3gxZztIlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer(output):\n",
        "    \"\"\"\n",
        "    Extract the answer from graph output.\n",
        "    Assumes `messages` list with last message as assistant reply.\n",
        "    \"\"\"\n",
        "    messages = output.get(\"messages\", [])\n",
        "    if not messages or len(messages) < 2:\n",
        "        return \"\"\n",
        "\n",
        "    assistant_message = messages[-1]\n",
        "    return assistant_message.get(\"content\", \"\") if isinstance(assistant_message, dict) else str(assistant_message)\n"
      ],
      "metadata": {
        "id": "zg6wW6hNPlgS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_evaluator(example, output):\n",
        "    expected = example[\"expected_output\"]\n",
        "    actual_output = extract_answer(output)\n",
        "\n",
        "    score = 1.0 if expected.lower() in actual_output.lower() else 0.0\n",
        "    comment = f\"✅ Correct\" if score == 1.0 else f\"❌ Expected '{expected}', got '{actual_output}'\"\n",
        "\n",
        "    return EvaluationResult(\n",
        "        key=\"accuracy\",           # This field was missing!\n",
        "        score=score,\n",
        "        comment=comment\n",
        "    )\n"
      ],
      "metadata": {
        "id": "HU7g2zY5UfkF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relevance_evaluator(example, output):\n",
        "    messages = output.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return EvaluationResult(\n",
        "            key=\"relevance\",\n",
        "            score=0.0,\n",
        "            comment=\"❌ No messages returned.\"\n",
        "        )\n",
        "\n",
        "    last_message = messages[-1]\n",
        "    content = last_message.get(\"content\", \"\")\n",
        "\n",
        "    if len(content) < 5:\n",
        "        return EvaluationResult(\n",
        "            key=\"relevance\",\n",
        "            score=0.0,\n",
        "            comment=\"❌ Answer too short.\"\n",
        "        )\n",
        "\n",
        "    return EvaluationResult(\n",
        "        key=\"relevance\",\n",
        "        score=1.0,\n",
        "        comment=\"✅ Reasonable response length.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "oUa9vUO8Ufn5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input examples 📄\n"
      ],
      "metadata": {
        "id": "HNhNsvjFuHoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"input\": {\"messages\": [{\"role\": \"user\", \"content\": \"What is the status of my order #1234?\"}]},\n",
        "        \"expected_output\": \"Your order #1234 has been shipped.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": {\"messages\": [{\"role\": \"user\", \"content\": \"Recommend me a smartwatch under $300\"}]},\n",
        "        \"expected_output\": \"smartwatches under $300\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "oxXzjbVHuGpB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "client = Client()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i2kaCKouGzi",
        "outputId": "62c91b9c-0817-4cd5-a2a3-687f499d5d32"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🔍 11. Run LangSmith Evaluations\n",
        "\n",
        "Evaluate a set of examples and log to LangSmith."
      ],
      "metadata": {
        "id": "YXisV2tyuaPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_name = \"multi-agent-graph-evaluation\"\n",
        "dataset_name = \"Smartwatch-Order-Dataset\"\n",
        "\n",
        "# Optional: Create dataset in LangSmith (if you want UI tracking)\n",
        "#dataset = client.create_dataset(\n",
        " #  dataset_name=dataset_name,\n",
        "  #description=\"Sample dataset for multi-agent LangGraph evaluation\"\n",
        "#)\n",
        "\n",
        "# Run evaluations and log to LangSmith\n",
        "for idx, example in enumerate(examples):\n",
        "    input_data = example[\"input\"]\n",
        "\n",
        "    # Run graph\n",
        "    with tracing_v2_enabled(project_name):\n",
        "        output = runnable_graph.invoke(input_data)\n",
        "\n",
        "    print(f\"\\n📝 Graph Output {idx}: {output}\")\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy_result = accuracy_evaluator(example=example, output=output)\n",
        "    relevance_result = relevance_evaluator(example, output)\n",
        "\n",
        "    # Combine evaluations\n",
        "    eval_result = {\n",
        "        \"accuracy_score\": accuracy_result.score,\n",
        "        \"accuracy_comment\": accuracy_result.comment,\n",
        "        \"relevance_score\": relevance_result.score,\n",
        "        \"relevance_comment\": relevance_result.comment\n",
        "    }\n",
        "\n",
        "    print(f\"✅ Eval Results for Example {idx}: {eval_result}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBohmvGjPloc",
        "outputId": "75bea5d1-8d31-4f95-ea48-747128da7889"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Memory Extraction Node: Processing message -> {'role': 'user', 'content': 'What is the status of my order #1234?'}\n",
            "🔎 RAG Node: Received question -> What is the status of my order #1234?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Summary Node triggered. Messages count: 2\n",
            "\n",
            "📝 Graph Output 0: {'messages': [{'role': 'user', 'content': 'What is the status of my order #1234?'}, {'role': 'assistant', 'content': '\"Samsung Gear S2 Review\". Samsung.'}], 'memory': {}}\n",
            "✅ Eval Results for Example 0: {'accuracy_score': 0.0, 'accuracy_comment': '❌ Expected \\'Your order #1234 has been shipped.\\', got \\'\"Samsung Gear S2 Review\". Samsung.\\'', 'relevance_score': 1.0, 'relevance_comment': '✅ Reasonable response length.'}\n",
            "📝 Memory Extraction Node: Processing message -> {'role': 'user', 'content': 'Recommend me a smartwatch under $300'}\n",
            "🔎 RAG Node: Received question -> Recommend me a smartwatch under $300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Summary Node triggered. Messages count: 2\n",
            "\n",
            "📝 Graph Output 1: {'messages': [{'role': 'user', 'content': 'Recommend me a smartwatch under $300'}, {'role': 'assistant', 'content': 'the minimum cost of smartphone-linked devices may be US$100.[60][61]'}], 'memory': {}}\n",
            "✅ Eval Results for Example 1: {'accuracy_score': 0.0, 'accuracy_comment': \"❌ Expected 'smartwatches under $300', got 'the minimum cost of smartphone-linked devices may be US$100.[60][61]'\", 'relevance_score': 1.0, 'relevance_comment': '✅ Reasonable response length.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = []\n",
        "\n",
        "for idx, example in enumerate(examples):\n",
        "    print(f\"\\n🔹 Running example {idx+1}...\")\n",
        "\n",
        "    # Run graph\n",
        "    state = example[\"input\"]\n",
        "    output = runnable_graph.invoke(state)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy_result = accuracy_evaluator(example, output)\n",
        "    relevance_result = relevance_evaluator(example, output)\n",
        "\n",
        "    # Print results\n",
        "    print(\"✅ Accuracy Eval:\", accuracy_result.dict())\n",
        "    print(\"✅ Relevance Eval:\", relevance_result.dict())\n",
        "\n",
        "    # Collect evaluations\n",
        "    evaluations.append({\n",
        "        \"example\": example,\n",
        "        \"output\": output,\n",
        "        \"accuracy\": accuracy_result,\n",
        "        \"relevance\": relevance_result\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFA2uDGJPJdo",
        "outputId": "a1a65185-1def-4e7d-d890-ba7fcec5e1f0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Running example 1...\n",
            "📝 Memory Extraction Node: Processing message -> {'role': 'assistant', 'content': '\"Samsung Gear S2 Review\". Samsung.'}\n",
            "🔎 RAG Node: Received question -> \"Samsung Gear S2 Review\". Samsung.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Summary Node triggered. Messages count: 3\n",
            "✅ Accuracy Eval: {'key': 'accuracy', 'score': 0.0, 'value': None, 'comment': \"❌ Expected 'Your order #1234 has been shipped.', got 'Samsung Gear S3 Frontier.[86]'\", 'correction': None, 'evaluator_info': {}, 'feedback_config': None, 'source_run_id': None, 'target_run_id': None, 'extra': None}\n",
            "✅ Relevance Eval: {'key': 'relevance', 'score': 1.0, 'value': None, 'comment': '✅ Reasonable response length.', 'correction': None, 'evaluator_info': {}, 'feedback_config': None, 'source_run_id': None, 'target_run_id': None, 'extra': None}\n",
            "\n",
            "🔹 Running example 2...\n",
            "📝 Memory Extraction Node: Processing message -> {'role': 'assistant', 'content': 'the minimum cost of smartphone-linked devices may be US$100.[60][61]'}\n",
            "🔎 RAG Node: Received question -> the minimum cost of smartphone-linked devices may be US$100.[60][61]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Summary Node triggered. Messages count: 3\n",
            "✅ Accuracy Eval: {'key': 'accuracy', 'score': 0.0, 'value': None, 'comment': \"❌ Expected 'smartwatches under $300', got 'the minimum cost of smartphone-linked devices may be US$100.[60][61]'\", 'correction': None, 'evaluator_info': {}, 'feedback_config': None, 'source_run_id': None, 'target_run_id': None, 'extra': None}\n",
            "✅ Relevance Eval: {'key': 'relevance', 'score': 1.0, 'value': None, 'comment': '✅ Reasonable response length.', 'correction': None, 'evaluator_info': {}, 'feedback_config': None, 'source_run_id': None, 'target_run_id': None, 'extra': None}\n"
          ]
        }
      ]
    }
  ]
}